{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7d1fa2",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Решающее-дерево\" data-toc-modified-id=\"Решающее-дерево-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Решающее дерево</a></span></li><li><span><a href=\"#CatBoostClassifier\" data-toc-modified-id=\"CatBoostClassifier-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>CatBoostClassifier</a></span></li></ul></li><li><span><a href=\"#Тестирование\" data-toc-modified-id=\"Тестирование-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Тестирование</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab825d96",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d035f",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629fbff",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73da6f",
   "metadata": {},
   "source": [
    "Загрузим нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f061a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импортируем библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.pipeline import Pipeline\n",
    "nltk.download('wordnet','stopwords','punkt','averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6506b",
   "metadata": {},
   "source": [
    "Прочитаем файл /datasets/toxic_comments.csv и сохраним его в переменную 'comment'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9608ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем файл\n",
    "comment = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717f167",
   "metadata": {},
   "source": [
    "Напишем функцию, которая выводит общую информацию о таблице, а так же посчитает пропуски и проверит на наличие дубликатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b45e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пишем функцию summary\n",
    "def summary(df):\n",
    "    print(df.info())\n",
    "    display(df.head())\n",
    "    count=0\n",
    "    for col in df.columns:\n",
    "        if df.isna().sum()[col]==0:\n",
    "            count+=1\n",
    "        else:\n",
    "            print(f'Пропущенных значений в столбце {col}: {df.isna().sum()[col]}')\n",
    "    if count==len(df.columns):\n",
    "        print('Пропущенных значений в таблице нет')\n",
    "    print(f'Повторов в таблице {df.duplicated().sum()}') \n",
    "    display(df.describe(percentiles=[.25,.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f84d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропущенных значений в таблице нет\n",
      "Повторов в таблице 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.302139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  159292.000000\n",
       "mean        0.101612\n",
       "std         0.302139\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "max         1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# смотрим на данные таблицы\n",
    "summary(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd828a",
   "metadata": {},
   "source": [
    "Всего имеется 159292 комментарий. Данные без пропусков и дубликатов. Так как перед нами стоит задача классификации, то проверим веса целевого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cceb885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8.841344371679229"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# исследуем веса целевого признака\n",
    "display(comment['toxic'].value_counts())\n",
    "class_frequency = comment['toxic'].value_counts()[0] / comment['toxic'].value_counts()[1]\n",
    "display(class_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede890d8",
   "metadata": {},
   "source": [
    "Веса не сбалансированны. Соотношение почти 9:1. При обучениее моделей, будем использовать изменение весов классов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70564f01",
   "metadata": {},
   "source": [
    "Для дальнейшей работы проведём лематизацию и очистку от лишних символов с помощью регулярных выражений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3415dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#выполним токенизациию и лемматизацию массива текстов\n",
    "L = WordNetLemmatizer()\n",
    "def lemmatizered(comment):\n",
    "    comment_new = []\n",
    "    for sentence in comment:\n",
    "        word_list = nltk.word_tokenize(sentence)\n",
    "        comment_new.append(' '.join([L.lemmatize(w) for w in word_list]))\n",
    "    return comment_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57491d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#возвращаем значение часть речи «part-of-speech» (POS-тег).\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    " \n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040631dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполним токенезацию и лемматизацию с учетом POS-тег\n",
    "def get_word_text(comment):\n",
    "    comment_new = []\n",
    "    for sentence in comment:\n",
    "        comment_new.append(' '.join([L.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence) if not w in stopwords]))\n",
    "    return comment_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48ee02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "816e25ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 42s, sys: 1min 11s, total: 14min 54s\n",
      "Wall time: 14min 55s\n"
     ]
    }
   ],
   "source": [
    " %%time\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "comment['lemm_text'] = get_word_text(comment['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e64b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = comment.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4e348",
   "metadata": {},
   "source": [
    "Подготовим данные для обучения моделей. Зададим признак и целевой признак. Разобъём исходный датасет на 3 выборки в пропорции 60/20/20(т.к. исходный датасет большого объёма). импортируем стоп-слова. Произведём векторизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c60742dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаём признаки и целевой признак\n",
    "target = comment['toxic']\n",
    "features = comment.drop(['toxic'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc6a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиваем исходный датасет на выборки\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.4, \n",
    "                                                                              random_state=12345)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, \n",
    "                                                                            target_valid, \n",
    "                                                                            test_size=0.5,\n",
    "                                                                            random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1007f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# загружаем стоп-слова\n",
    "# создаём счетчик, указав в нём стоп-слова\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "157b01a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95575, 131678)\n",
      "(31858, 131678)\n",
      "(31859, 131678)\n"
     ]
    }
   ],
   "source": [
    "# посчитаем tf-idf  для признаков\n",
    "features_train = count_tf_idf.fit_transform(features_train['lemm_text'])\n",
    "features_valid = count_tf_idf.transform(features_valid['lemm_text'])\n",
    "features_test = count_tf_idf.transform(features_test['lemm_text'])\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7502e",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "* были загружены данные для обучения модели\n",
    "* был исследован баланс классов целевого признака\n",
    "* принято решение о борьбе с дисбаланосм с помощью взвешивания классов\n",
    "* была произведена лемматизация и удалены исходные твиты\n",
    "* были заданы признаки и целевой признак\n",
    "* произведена векторизация признаков\n",
    "* данные разбиты на выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459cae3",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c16b14",
   "metadata": {},
   "source": [
    "Для обучения выберем следующие модели:\n",
    "\n",
    " * LogisticRegression\n",
    " * DecisionTreeClassifier\n",
    " * CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ec489",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb12da4",
   "metadata": {},
   "source": [
    "Найдём гиперпараметры для логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20067d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes={0:1, 1:class_frequency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e85b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры для регрессии:\n",
      "\n",
      "{'C': 10, 'class_weight': {0: 1, 1: 8.841344371679229}, 'solver': 'lbfgs'}\n",
      "CPU times: user 6min 43s, sys: 7min 54s, total: 14min 37s\n",
      "Wall time: 14min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "hyperparams = [{'solver':['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'C':[0.1, 1, 10],\n",
    "                'class_weight':[dict_classes]}]\n",
    "clf = GridSearchCV(model_lr, hyperparams, scoring='f1',cv=4)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"лучшие параметры для регрессии:\")\n",
    "print()\n",
    "lr_best_params = clf.best_params_\n",
    "print(lr_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5ee1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на валидации 0.7560097883978696\n",
      "CPU times: user 19.6 s, sys: 23.4 s, total: 43 s\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.set_params(**lr_best_params)\n",
    "model_lr.fit(features_train, target_train)\n",
    "target_predict = model_lr.predict(features_valid)\n",
    "valid_f1_lr = f1_score(target_valid, target_predict)\n",
    "print('F1 на валидации', valid_f1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ea227",
   "metadata": {},
   "source": [
    "### Решающее дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00df6079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для решающего дерева:\n",
      "\n",
      "{'class_weight': {0: 1, 1: 8.841344371679229}, 'max_depth': 35, 'random_state': 12345}\n",
      "CPU times: user 19min 8s, sys: 11.3 s, total: 19min 20s\n",
      "Wall time: 19min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_dtс = DecisionTreeClassifier()\n",
    "hyperparams = [{'max_depth':[x for x in range(9,45,2)],\n",
    "                'random_state':[12345],\n",
    "                'class_weight':[dict_classes]}]\n",
    "\n",
    "clf = GridSearchCV(model_dtс , hyperparams, scoring='f1',cv=4)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Лучшие параметры для решающего дерева:\")\n",
    "print()\n",
    "dtc_best_params = clf.best_params_\n",
    "print(dtc_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ea4dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 для решающего дерева 0.6268364348677767\n",
      "CPU times: user 22 s, sys: 140 ms, total: 22.2 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_dtc = DecisionTreeClassifier()\n",
    "model_dtc.set_params(**dtc_best_params)\n",
    "model_dtc.fit(features_train, target_train)\n",
    "target_predict = model_dtc.predict(features_valid)\n",
    "valid_f1_dtc = f1_score(target_valid, target_predict)\n",
    "print('F1 для решающего дерева', valid_f1_dtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476f67f",
   "metadata": {},
   "source": [
    "### CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84bf92d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 для CatBoostClassifier 0.7380696088984572\n",
      "CPU times: user 25min 28s, sys: 6min 27s, total: 31min 55s\n",
      "Wall time: 31min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_cbc = CatBoostClassifier(verbose=False, iterations=200)\n",
    "model_cbc.fit(features_train, target_train)\n",
    "target_predict_cbc = model_cbc.predict(features_valid)\n",
    "cv_f1_cbc = cross_val_score(model_cbc,\n",
    "                                         features_train, \n",
    "                                         target_train, \n",
    "                                         cv=4, \n",
    "                                         scoring='f1').mean()\n",
    "valid_f1_cbc = f1_score(target_valid, target_predict_cbc)\n",
    "print('F1 для CatBoostClassifier', valid_f1_cbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c2dd2",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "* были обучены 3 модели\n",
    "* по итогам обучения метрика F-1 соответсвует требованиям у модели логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3218d",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370e415",
   "metadata": {},
   "source": [
    "Проведём тестирование моделей LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af74bbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F-1 модели LogisticRegression:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7473668595163923"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# находим F-1 меру тестовой выборки\n",
    "decision_model_lr = LogisticRegression(**lr_best_params)\n",
    "decision_model_lr.fit(features_train, target_train)\n",
    "test_predictions_lr = decision_model_lr.predict(features_test)\n",
    "f1_score_decision_modelt_test_lr = f1_score(target_test, test_predictions_lr)\n",
    "display('F-1 модели LogisticRegression:', f1_score_decision_modelt_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618db528",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24e8f3",
   "metadata": {},
   "source": [
    "* В ходе исселодования была найдена наилучшая модель для определения токсичности твитов - модель LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5bfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 5202,
    "start_time": "2022-10-24T20:48:06.645Z"
   },
   {
    "duration": 2101,
    "start_time": "2022-10-24T20:48:35.364Z"
   },
   {
    "duration": 4,
    "start_time": "2022-10-24T20:49:13.591Z"
   },
   {
    "duration": 311,
    "start_time": "2022-10-24T20:49:22.015Z"
   },
   {
    "duration": 7,
    "start_time": "2022-10-24T20:51:38.633Z"
   },
   {
    "duration": 3407,
    "start_time": "2022-10-24T21:05:18.115Z"
   },
   {
    "duration": 735,
    "start_time": "2022-10-24T21:05:21.523Z"
   },
   {
    "duration": 4,
    "start_time": "2022-10-24T21:05:22.260Z"
   },
   {
    "duration": 289,
    "start_time": "2022-10-24T21:05:22.265Z"
   },
   {
    "duration": 10,
    "start_time": "2022-10-24T21:06:55.445Z"
   },
   {
    "duration": 3,
    "start_time": "2022-10-24T21:08:24.874Z"
   },
   {
    "duration": 4,
    "start_time": "2022-10-24T21:08:38.654Z"
   },
   {
    "duration": 3,
    "start_time": "2022-10-24T21:08:46.774Z"
   },
   {
    "duration": 5,
    "start_time": "2022-10-24T21:08:56.085Z"
   },
   {
    "duration": 226732,
    "start_time": "2022-10-24T21:09:12.455Z"
   },
   {
    "duration": 6196,
    "start_time": "2022-10-25T12:57:43.050Z"
   },
   {
    "duration": 3084,
    "start_time": "2022-10-25T12:57:49.248Z"
   },
   {
    "duration": 4,
    "start_time": "2022-10-25T12:57:52.333Z"
   },
   {
    "duration": 310,
    "start_time": "2022-10-25T12:57:52.339Z"
   },
   {
    "duration": 10,
    "start_time": "2022-10-25T12:57:52.651Z"
   },
   {
    "duration": 3,
    "start_time": "2022-10-25T12:57:52.663Z"
   },
   {
    "duration": 11,
    "start_time": "2022-10-25T12:57:52.668Z"
   },
   {
    "duration": 8,
    "start_time": "2022-10-25T12:57:52.680Z"
   },
   {
    "duration": 18,
    "start_time": "2022-10-25T12:57:52.689Z"
   },
   {
    "duration": 729161,
    "start_time": "2022-10-25T12:57:52.709Z"
   },
   {
    "duration": 5,
    "start_time": "2022-10-25T13:10:01.872Z"
   },
   {
    "duration": 43,
    "start_time": "2022-10-25T13:10:01.878Z"
   },
   {
    "duration": 52,
    "start_time": "2022-10-25T13:10:01.922Z"
   },
   {
    "duration": 51,
    "start_time": "2022-10-25T13:10:01.977Z"
   },
   {
    "duration": 1524,
    "start_time": "2022-10-25T13:10:02.030Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-25T13:10:03.556Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-25T13:10:03.557Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-25T13:10:03.558Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-25T13:10:03.559Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-25T13:10:03.560Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-25T13:10:03.561Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-25T13:10:03.562Z"
   },
   {
    "duration": 5419,
    "start_time": "2022-10-25T19:02:53.500Z"
   },
   {
    "duration": 2243,
    "start_time": "2022-10-25T19:02:58.921Z"
   },
   {
    "duration": 4,
    "start_time": "2022-10-25T19:03:01.166Z"
   },
   {
    "duration": 327,
    "start_time": "2022-10-25T19:03:01.171Z"
   },
   {
    "duration": 9,
    "start_time": "2022-10-25T19:03:01.500Z"
   },
   {
    "duration": 4,
    "start_time": "2022-10-25T19:03:01.511Z"
   },
   {
    "duration": 24,
    "start_time": "2022-10-25T19:03:01.516Z"
   },
   {
    "duration": 6,
    "start_time": "2022-10-25T19:03:01.541Z"
   },
   {
    "duration": 8,
    "start_time": "2022-10-25T19:03:01.549Z"
   },
   {
    "duration": 895029,
    "start_time": "2022-10-25T19:03:01.559Z"
   },
   {
    "duration": 54,
    "start_time": "2022-10-25T19:17:56.590Z"
   },
   {
    "duration": 11,
    "start_time": "2022-10-25T19:17:56.646Z"
   },
   {
    "duration": 33,
    "start_time": "2022-10-25T19:17:56.659Z"
   },
   {
    "duration": 5,
    "start_time": "2022-10-25T19:17:56.694Z"
   },
   {
    "duration": 5680,
    "start_time": "2022-10-25T19:17:56.700Z"
   },
   {
    "duration": 3,
    "start_time": "2022-10-25T19:18:02.382Z"
   },
   {
    "duration": 878853,
    "start_time": "2022-10-25T19:18:02.387Z"
   },
   {
    "duration": 43196,
    "start_time": "2022-10-25T19:32:41.243Z"
   },
   {
    "duration": 1160804,
    "start_time": "2022-10-25T19:33:24.441Z"
   },
   {
    "duration": 22217,
    "start_time": "2022-10-25T19:52:45.247Z"
   },
   {
    "duration": 1919591,
    "start_time": "2022-10-25T19:53:07.465Z"
   },
   {
    "duration": 43696,
    "start_time": "2022-10-25T20:25:07.057Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
